{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac1dc955",
   "metadata": {},
   "source": [
    "This is the analysis notebook to reproduce the data in section 2.2. of the thesis.\n",
    "\n",
    "It answers the following questions:\n",
    "\n",
    "* pages and revisions:\n",
    "    * how did the number of pages in enwiktionary evolve over time?\n",
    "    * when were pages last edited (are there 'zombie' pages)?\n",
    "    * revisions per page\n",
    "* contributors:\n",
    "    * what is the contribution of bots?\n",
    "    * how big is the contributor community?\n",
    "* languages:\n",
    "    * how many languages per entry in most recent slice and over time \n",
    "    * how many entries with at least one etymology section in most recent slice and over time\n",
    "* templates:\n",
    "    * which are the most common templates in etymology sections over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16c06d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.getLogger(\"matplotlib\").setLevel(logging.WARNING)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"HistoricalAnalysis\")\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import math\n",
    "import multiprocessing as mp\n",
    "from typing import List, Callable\n",
    "from functools import reduce\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker, colors\n",
    "\n",
    "import etymmap.specific_en\n",
    "\n",
    "etymmap.specific_en.configure()\n",
    "\n",
    "from etymmap.wiktionary import DumpProcessor, raw2json\n",
    "from etymmap.analyze import inspect_json_paths, StatisticsWriter, StatisticsReader\n",
    "from etymmap.specific import Specific\n",
    "\n",
    "from utils import cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5d30d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DUMP = DumpProcessor(\"../dumps/enwiktionary-20220601-pages-meta-history.xml.bz2\")\n",
    "DATA_PATH = Path(\"./data/enwiktionary-20220601-pages-meta-history\")\n",
    "reader = StatisticsReader(DATA_PATH)\n",
    "\n",
    "MIN_DATE = \"2001-04-01\"\n",
    "MAX_DATE = \"2022-07-01\"\n",
    "\n",
    "monthly_timebins = pd.date_range(MIN_DATE, MAX_DATE, freq=\"MS\")\n",
    "quarterly_timebins = pd.date_range(MIN_DATE, MAX_DATE, freq=\"QS\")\n",
    "annual_timebins = pd.date_range(\"2001\", \"2023\", freq=\"YS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53be606b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparation\n",
    "\n",
    "\n",
    "def get_bot_ids(reader: StatisticsReader):\n",
    "    contributors = reader.contributors()\n",
    "    bot_tables = pd.read_html(\"https://en.wiktionary.org/wiki/Wiktionary:Bots\")\n",
    "    bot_names = pd.concat(\n",
    "        [table[\"Bot name\"].str.replace(r\"\\(.*\\)\", \"\") for table in bot_tables[1:3]],\n",
    "        ignore_index=True,\n",
    "    ).str.strip()\n",
    "    return (\n",
    "        contributors[contributors.username.isin(bot_names)].id.astype(int).astype(str)\n",
    "    )\n",
    "\n",
    "\n",
    "def prepare_revisions(reader: StatisticsReader):\n",
    "    \"\"\"\n",
    "    Sort the revisions by page_id and timestamp and set page_id as index\n",
    "    \"\"\"\n",
    "    revisions = pd.concat(\n",
    "        list(\n",
    "            tqdm(\n",
    "                reader.revisions(\n",
    "                    iterator=True,\n",
    "                    chunksize=10**6,\n",
    "                    # put sort columns to the left\n",
    "                    usecols=[\"id\", \"contributor\", \"minor\", \"page_id\", \"timestamp\"],\n",
    "                ),\n",
    "                total=66,\n",
    "                unit=\" chunks\",\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    revisions.sort_values([\"page_id\", \"timestamp\"], inplace=True)\n",
    "    revisions.set_index(\"page_id\", inplace=True)\n",
    "    return revisions\n",
    "\n",
    "\n",
    "# utility\n",
    "\n",
    "\n",
    "def number_format(x, pos):\n",
    "    for k, (s, r) in {10**6: (\"M\", 1), 1000: (\"k\", 0), 0: (\"\", 0)}.items():\n",
    "        if x >= k:\n",
    "            if r:\n",
    "                return f\"{round(x / k, r)}{s}\"\n",
    "            return f\"{int(x) // (k or 1)}{s}\"\n",
    "    return x\n",
    "\n",
    "\n",
    "numberFormatter = ticker.FuncFormatter(number_format)\n",
    "\n",
    "bot_ids = cached(lambda: get_bot_ids(reader), DATA_PATH / \"bot_ids.pickle\")\n",
    "\n",
    "\n",
    "def read_main_ns_page_ids():\n",
    "    pages = reader.pages(usecols=[\"id\", \"namespace\"])\n",
    "    return pages[pages.namespace.isin([0, 118])].id\n",
    "\n",
    "\n",
    "def last_idx(size):\n",
    "    ret = np.zeros(size, dtype=bool)\n",
    "    ret[-1] = True\n",
    "    return ret\n",
    "\n",
    "\n",
    "def get_revisions(\n",
    "    only_major=True,\n",
    "    keep_last=True,\n",
    "    only_main=True,\n",
    "    only_human=False,\n",
    "    only_last_per_timebin=False,\n",
    "    timebins=monthly_timebins,\n",
    "    head=None,\n",
    "    columns=None,\n",
    "):\n",
    "    revisions = pd.read_pickle(DATA_PATH / \"revisions.pickle\").head(head)\n",
    "\n",
    "    if only_main:\n",
    "        logger.info(f\"Filter main namespace revisions ({len(revisions)})\")\n",
    "        main_ns_page_ids = read_main_ns_page_ids()\n",
    "        revisions = revisions[revisions.index.isin(main_ns_page_ids)]\n",
    "        logger.info(f\"Keep {len(revisions)}\")\n",
    "\n",
    "    if only_human:\n",
    "        logger.info(f\"Filter bot revisions ({len(revisions)})\")\n",
    "        revisions = revisions[~revisions.contributor.isin(bot_ids)]\n",
    "        logger.info(f\"Keep {len(revisions)}\")\n",
    "\n",
    "    if only_major:\n",
    "        logger.info(f\"Filter minor revisions ({len(revisions)})\")\n",
    "        if keep_last:\n",
    "            last = revisions.groupby(\"page_id\").tail(1).id\n",
    "            revisions = revisions[~revisions.minor | revisions.id.isin(last)]\n",
    "        else:\n",
    "            revisions = revisions[~revisions.minor]\n",
    "        logger.info(f\"Keep {len(revisions)}\")\n",
    "\n",
    "    revisions[\"timebin\"] = pd.cut(\n",
    "        revisions.timestamp, bins=timebins, labels=timebins[:-1]\n",
    "    )\n",
    "    if only_last_per_timebin:\n",
    "        logger.info(f\"Filter last per timebin ({len(revisions)})\")\n",
    "        # we can use the tail as timestamps are sorted\n",
    "        revisions = revisions.groupby([\"page_id\", \"timebin\"], observed=True).tail(1)\n",
    "        logger.info(f\"Keep {len(revisions)}\")\n",
    "    return revisions[columns] if columns else revisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e40f845",
   "metadata": {},
   "source": [
    "# Compile the statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c943f61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes ~4 hours with 8 cores intel i7\n",
    "\n",
    "# writer = StatisticsWriter()\n",
    "# writer.write_stats(DUMP,\n",
    "#                   out=\"./data/enwiktionary-20220601-pages-meta-history\",\n",
    "#                   chunksize_bytes=500*10**6,\n",
    "#                   progress={\"unit\": \"pages\", \"total\": 8.16*10**6},\n",
    "#                   mp_processes=6)\n",
    "\n",
    "\n",
    "# ! requires lot of memory for sorting (~ 10 GB)\n",
    "_ = cached(prepare_revisions, DATA_PATH / \"revisions.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c090f19",
   "metadata": {},
   "source": [
    "## Page statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3de440",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = reader.pages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c248b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total number of pages?\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68c73b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of pages per namespace?\n",
    "hist = pages.namespace.value_counts()\n",
    "pd.DataFrame({\"count\": hist, \"%\": round(100 * hist / len(pages), 3)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebaff28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creation, last edited, number of revisions per page\n",
    "page_stats = (\n",
    "    get_revisions(only_main=True, only_human=False, only_major=True, keep_last=True)\n",
    "    .timestamp.groupby(level=\"page_id\", sort=False)\n",
    "    .agg([\"min\", \"max\", \"count\"])\n",
    "    .rename(columns={\"min\": \"created\", \"max\": \"last_edited\", \"count\": \"n_revisions\"})\n",
    ")\n",
    "\n",
    "page_stats[\"created_bins\"] = pd.cut(\n",
    "    page_stats.created, bins=monthly_timebins, labels=monthly_timebins[:-1]\n",
    ")\n",
    "page_stats[\"last_edited_bins\"] = pd.cut(\n",
    "    page_stats.last_edited, bins=monthly_timebins, labels=monthly_timebins[:-1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c0dee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of pages over time and last edit histogram\n",
    "created = page_stats.created_bins.value_counts().sort_index()\n",
    "last_edited = page_stats.last_edited_bins.value_counts().sort_index()\n",
    "created.index = pd.to_datetime(created.index).date\n",
    "last_edited.index = pd.to_datetime(last_edited.index).date\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "xticks = pd.date_range(\"2001\", \"2022\", freq=pd.DateOffset(years=3))\n",
    "created.cumsum().plot(ax=axes[0], grid=True, xticks=xticks)\n",
    "last_edited.plot(drawstyle=\"steps\", ax=axes[1], grid=True, xticks=xticks)\n",
    "for a in axes:\n",
    "    a.set_xticklabels(xticks.year)\n",
    "    a.yaxis.set_major_formatter(numberFormatter)\n",
    "plt.savefig(\"pages.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269c2c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of major edits per month\n",
    "increase_per_month = page_stats.last_edited_bins.value_counts()\n",
    "increase_per_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed619e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_active_users(months):\n",
    "    ret = []\n",
    "    contributors = reader.contributors()\n",
    "    contributors = contributors[contributors.id.notnull()]\n",
    "    contributors[\"id\"] = contributors.id.astype(int).astype(str)\n",
    "    revisions = get_revisions(\n",
    "        only_main=True, only_human=False, only_major=True, keep_last=True\n",
    "    )\n",
    "    for month in months:\n",
    "        revisions_of_month = revisions[revisions.timebin == month]\n",
    "        edits_per_contributor = revisions_of_month.contributor.value_counts()\n",
    "        user = edits_per_contributor.index[0]\n",
    "        username = contributors[contributors.id == user].username.iloc[0]\n",
    "        number_of_edits = edits_per_contributor[0]\n",
    "        ratio = round(100 * number_of_edits / len(revisions_of_month), 3)\n",
    "        ret.append((month, user, username, number_of_edits, ratio))\n",
    "    return pd.DataFrame.from_records(\n",
    "        ret, columns=[\"month\", \"id\", \"username\", \"n_edits\", \"% month\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027fb9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_active_users(increase_per_month.index[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02b8667",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_revisions = get_revisions(\n",
    "    only_main=True, only_human=False, only_major=False, columns=[\"timebin\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61bdd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_bot_revisions = get_revisions(\n",
    "    only_main=True, only_human=True, only_major=False, columns=[\"timebin\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615cdbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how much of the pages are edited by bots?\n",
    "round(1 - (len(non_bot_revisions) / len(all_revisions)), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b64e704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many pages are only edited by bots?\n",
    "round(1 - non_bot_revisions.index.unique().size / all_revisions.index.unique().size, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643d3760",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_stats_nonbots = non_bot_revisions.timebin.groupby(\"page_id\").agg(\n",
    "    [\"min\", \"max\", \"count\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8fde11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# last edits for nonbots\n",
    "last_edited = (\n",
    "    page_stats_nonbots[\"max\"].value_counts().rename(\"last_edited\").sort_index()\n",
    ")\n",
    "last_edited.index = pd.to_datetime(last_edited.index).date\n",
    "last_edited.plot(drawstyle=\"steps\", figsize=(14, 6), grid=True, xticks=xticks)\n",
    "_ = plt.gca().set_xticklabels(xticks.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec38ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same, but cumsum\n",
    "(last_edited.cumsum() / last_edited.sum()).plot(grid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78160dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x-axis: number of edits, y-axis: number of users that made this many edits\n",
    "data = page_stats_nonbots[\"count\"].hist(bins=range(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5fcb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most (human) edited pages\n",
    "(\n",
    "    page_stats_nonbots[[\"count\"]]\n",
    "    .sort_values(\"count\", ascending=False)\n",
    "    .merge(\n",
    "        reader.pages(usecols=[\"id\", \"title\"]),\n",
    "        how=\"left\",\n",
    "        left_on=\"page_id\",\n",
    "        right_on=\"id\",\n",
    "    )\n",
    ").head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f466b8",
   "metadata": {},
   "source": [
    "# Contributors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ac0bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many total contributors?\n",
    "len(\n",
    "    get_revisions(\n",
    "        only_main=False, only_human=False, only_major=False, columns=[\"contributor\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c0bf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# revisions per contributor\n",
    "revisions = get_revisions(\n",
    "    only_main=True,\n",
    "    only_human=False,\n",
    "    only_major=False,\n",
    "    columns=[\"timebin\", \"contributor\", \"minor\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54348185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_stats(d):\n",
    "    return pd.Series(\n",
    "        {\n",
    "            \"contributions\": len(d),\n",
    "            \"edited_pages\": d.index.unique().size,\n",
    "            \"major_contributions\": len(d[~d.minor]),\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984e3312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contributor_stats = revisions.groupby(\"contributor\").progress_apply(get_user_stats)\n",
    "# contributor_stats.to_pickle(DATA_PATH / \"contributor_stats.pickle\")\n",
    "contributor_stats = pd.read_pickle(DATA_PATH / \"contributor_stats.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e9e503",
   "metadata": {},
   "outputs": [],
   "source": [
    "contributor_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfb5771",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = contributor_stats.contributions.sum()\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7462b221",
   "metadata": {},
   "outputs": [],
   "source": [
    "contributor_stats[\"is_bot\"] = pd.Series(\n",
    "    contributor_stats.index.isin(bot_ids), index=contributor_stats.index\n",
    ").replace({True: 1, False: -1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00ac629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most contributors have very few edits\n",
    "contributor_stats.contributions.hist(bins=range(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5700f64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "contributors = reader.contributors()\n",
    "contributors.id = contributors.id.fillna(-1).astype(int).astype(str)\n",
    "contributor_stats = contributor_stats.merge(\n",
    "    contributors[[\"id\", \"username\"]], left_index=True, right_on=\"id\", how=\"left\"\n",
    ")\n",
    "contributor_stats.set_index(\"id\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a40eccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "contributor_stats[contributor_stats.is_bot == -1].sort_values(\n",
    "    \"contributions\", ascending=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42870af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe_bot = (contributor_stats.is_bot == -1) & contributor_stats.username.str.match(\n",
    "    \".*[Bb]ot$\"\n",
    ").fillna(False)\n",
    "contributor_stats.is_bot.loc[maybe_bot] = 0\n",
    "\n",
    "contributor_stats.is_bot.loc[contributor_stats.username.isna()] = -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de66061",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n",
    "by_group = contributor_stats.groupby(\"is_bot\").contributions.sum() / total\n",
    "by_group.index = [\"unregistered\", \"human\", \"maybe bot\", \"bot\"]\n",
    "by_group.plot(kind=\"bar\", ax=axs[0])\n",
    "axs[0].set_xticklabels(by_group.index.values, rotation=0)\n",
    "axs[0].grid(axis=\"y\")\n",
    "\n",
    "topk = [10, 100, 1000, 100000]\n",
    "contributions = contributor_stats[\n",
    "    contributor_stats.is_bot < 0\n",
    "].contributions.sort_values(ascending=False)\n",
    "topk_ratios = pd.Series(\n",
    "    {i: contributions.iloc[:i].sum() / contributions.sum() for i in topk}\n",
    ")\n",
    "topk_ratios.plot(ax=axs[1], kind=\"bar\")\n",
    "axs[1].grid(axis=\"y\")\n",
    "_ = axs[1].set_xticklabels(topk, rotation=0)\n",
    "plt.savefig(\"users.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5f469f",
   "metadata": {},
   "source": [
    "# Languages and Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caafbfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NWORKERS = 8\n",
    "SPLIT = 512\n",
    "BINS = annual_timebins\n",
    "\n",
    "\n",
    "def split_entries(entries, n=8):\n",
    "    groups = entries.groupby(\"page_id\").ngroup()\n",
    "    group_chunks = groups // math.ceil(groups.iloc[-1] / n)\n",
    "    starts = [(group_chunks == i).argmax() for i in range(n)]\n",
    "    for start, end in zip(starts, starts[1:] + [len(entries)]):\n",
    "        yield entries.iloc[start:end]\n",
    "\n",
    "\n",
    "def prepare_entries(\n",
    "    reader, columns=[\"revision_id\", \"language\", \"has_etymology\"], split=True\n",
    "):\n",
    "    logger.info(\"Read and filter revisions\")\n",
    "    revisions = get_revisions(\n",
    "        only_last_per_timebin=True, columns=[\"id\", \"timebin\"], timebins=BINS\n",
    "    )\n",
    "\n",
    "    logger.info(\"Read entries\")\n",
    "    langs = reader.entries(usecols=columns, index_col=\"revision_id\")\n",
    "\n",
    "    logger.info(\"Merging...\")\n",
    "    entries = revisions.merge(langs, left_on=\"id\", right_index=True, how=\"left\")\n",
    "    logger.info(\"Timebins as datetime\")\n",
    "    entries.timebin = pd.to_datetime(entries.timebin)\n",
    "\n",
    "    if split:\n",
    "        logger.info(f\"Split into {SPLIT} frames\")\n",
    "        return list(split_entries(entries, n=SPLIT))\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb55dc7",
   "metadata": {},
   "source": [
    "## Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd7f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = prepare_entries(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8616834",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([e.memory_usage() for e in entries]).sum() / 10**9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f56d317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def languages_per_page(page_df):\n",
    "    return (\n",
    "        page_df[[\"timebin\", \"language\"]]\n",
    "        .groupby(\"timebin\")\n",
    "        .language\n",
    "        # collect all languages in an array per timebin\n",
    "        .apply(lambda s: (None if s.empty else s.values))\n",
    "        # we need a forward fill for all timebins\n",
    "        .reindex(BINS)\n",
    "        .fillna(method=\"ffill\")\n",
    "        .explode()\n",
    "    )\n",
    "\n",
    "\n",
    "def etym_languages_per_page(page_df):\n",
    "    return (\n",
    "        page_df[[\"timebin\", \"language\", \"has_etymology\"]]\n",
    "        .groupby(\"timebin\")[[\"language\", \"has_etymology\"]]\n",
    "        # collect all languages in an array per timebin\n",
    "        # don't ask me why pandas cannot handle the array as above\n",
    "        .apply(\n",
    "            lambda df: (None if df.empty else df[df.has_etymology].language.tolist())\n",
    "        )\n",
    "        # we need a forward fill for all timebins\n",
    "        .reindex(BINS)\n",
    "        .fillna(method=\"ffill\")\n",
    "        .explode()\n",
    "    )\n",
    "\n",
    "\n",
    "def languages_count(_entries, with_etym=False):\n",
    "    return (\n",
    "        _entries.groupby(\"page_id\")\n",
    "        .apply(etym_languages_per_page if with_etym else languages_per_page)\n",
    "        .groupby(level=\"timebin\")\n",
    "        .value_counts()\n",
    "        .unstack(level=\"language\")\n",
    "    )\n",
    "\n",
    "\n",
    "def etym_languages_count(_entries):\n",
    "    return languages_count(_entries, True)\n",
    "\n",
    "\n",
    "def chunkwise_languages_count(\n",
    "    entries: List[pd.DataFrame], count_chunk: Callable = languages_count\n",
    "):\n",
    "    with mp.Pool(NWORKERS) as pool:\n",
    "        counts = list(tqdm(pool.imap_unordered(count_chunk, entries), total=SPLIT))\n",
    "    all_counts = pd.DataFrame(\n",
    "        0, index=BINS, columns=set(c for d in counts for c in d.columns)\n",
    "    )\n",
    "    for count in counts:\n",
    "        all_counts = all_counts.add(count, fill_value=0)\n",
    "    languages = all_counts.iloc[-1].sort_values(ascending=False)\n",
    "    return all_counts[languages.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cced139",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT = 512\n",
    "BINS = monthly_timebins\n",
    "\n",
    "# language_counts = chunkwise_languages_count(entries[:8])\n",
    "# language_counts.to_pickle(DATA_PATH / \"language_counts.pickle\")\n",
    "language_counts = pd.read_pickle(DATA_PATH / \"language_counts.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019ca05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# etym_language_counts = chunkwise_language_count(entries, etym_languages_count)\n",
    "# etym_language_counts.to_pickle(DATA_PATH / \"etym_language_counts.pickle\")\n",
    "etym_language_counts = pd.read_pickle(DATA_PATH / \"etym_language_counts.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5115a98",
   "metadata": {},
   "source": [
    "## Current most common languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eff2d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = Specific.language_mapper\n",
    "\n",
    "current_lcs = language_counts.iloc[-1]\n",
    "current_elcs = etym_language_counts.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c885fddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_lcs.sum(), current_lcs[\n",
    "    ~pd.Series(current_lcs.index, index=current_lcs.index).str.startswith(\"?\")\n",
    "].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b6ad43",
   "metadata": {},
   "outputs": [],
   "source": [
    "top30 = current_lcs.astype(int).iloc[[*range(1, 31), 100, 500]].copy()\n",
    "top30.index = [lm.code2name(l) for l in top30.index]\n",
    "top30.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14948dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_lcs[current_lcs <= 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ec7cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rare = current_lcs[current_lcs <= 10].index[0]\n",
    "lm.code2name(rare), (current_lcs.index == rare).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4c3fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "(~pd.Series(current_lcs.index).str.startswith(\"?\")).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6ee3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(current_lcs[current_lcs >= i]) for i in [1000, 100, 10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaf9ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_lcs[\n",
    "    ~pd.Series(current_lcs.index, index=current_lcs.index).str.startswith(\"?\")\n",
    "].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9443ef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_elcs.sum(), current_elcs[\n",
    "    ~pd.Series(current_elcs.index, index=current_elcs.index).str.startswith(\"?\")\n",
    "].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62386f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "top30 = current_elcs.astype(int).iloc[[*range(50), 500]].copy()\n",
    "top30.index = [lm.code2name(l) for l in top30.index]\n",
    "top30.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11110bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rare = current_elcs[current_elcs <= 10].index[0]\n",
    "lm.code2name(rare), (current_elcs.index == rare).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475ea3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(current_elcs[current_elcs >= i]) for i in [1000, 100, 10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e06c42",
   "metadata": {},
   "source": [
    "# Languages over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882d799a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "default_colors = plt.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "langcolor = {\n",
    "    l: default_colors[i % len(default_colors)]\n",
    "    for i, l in enumerate(language_counts.columns[1:])\n",
    "}\n",
    "\n",
    "language_counts.iloc[:, 1:11].plot(\n",
    "    ax=axs[0],\n",
    "    color=[langcolor.get(c, default_colors[-1]) for c in language_counts.columns[1:11]],\n",
    ")\n",
    "axs[0].grid(axis=\"y\")\n",
    "axs[0].yaxis.set_major_formatter(numberFormatter)\n",
    "\n",
    "etym_language_counts.iloc[:, :10].plot(\n",
    "    ax=axs[1],\n",
    "    color=[\n",
    "        langcolor.get(c, default_colors[-1]) for c in etym_language_counts.columns[:10]\n",
    "    ],\n",
    ")\n",
    "axs[1].grid(axis=\"y\")\n",
    "axs[1].yaxis.set_major_formatter(numberFormatter)\n",
    "plt.savefig(\"languages.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c613906a",
   "metadata": {},
   "source": [
    "## Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18e0eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT = 1024\n",
    "BINS = quarterly_timebins\n",
    "\n",
    "entries = prepare_entries(\n",
    "    reader, columns=[\"revision_id\", \"language\", \"templates\", \"has_etymology\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ee6483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine(per_timebin):\n",
    "    def combine_(s):\n",
    "        return reduce(list.__iadd__, s)\n",
    "\n",
    "    if per_timebin.empty:\n",
    "        return None\n",
    "    else:\n",
    "        per_timebin = per_timebin[\n",
    "            per_timebin.templates.notnull() & per_timebin.has_etymology\n",
    "        ]\n",
    "        if per_timebin.empty:\n",
    "            # the entry at this timebin does not have etymology templates\n",
    "            return []\n",
    "        elif len(per_timebin) == 1:\n",
    "            return per_timebin.templates.iloc[0]\n",
    "        else:\n",
    "            # only sometimes, there are multiple entries of the same language\n",
    "            # then build one list containing all elements\n",
    "            return per_timebin.templates.agg(combine_)\n",
    "\n",
    "\n",
    "def combine_templates(df):\n",
    "    return df.groupby([\"page_id\", \"language\", \"timebin\"], observed=True)[\n",
    "        [\"templates\", \"has_etymology\"]\n",
    "    ].apply(combine)\n",
    "\n",
    "\n",
    "def ffill(df):\n",
    "    return df.groupby([\"page_id\", \"language\"]).apply(\n",
    "        lambda s: pd.Series(\n",
    "            s.droplevel([\"page_id\", \"language\"]), index=named_index\n",
    "        ).fillna(method=\"ffill\")\n",
    "    )\n",
    "\n",
    "\n",
    "named_index = pd.Series(BINS, name=\"timebin\")\n",
    "\n",
    "\n",
    "def etymology_template_counts(chunk: pd.DataFrame):\n",
    "    chunk[\"templates\"] = chunk.templates.replace(\"\", None).str.split(\"|\")\n",
    "    templates_per_timebin = (ffill(combine_templates(chunk))).droplevel(\n",
    "        [\"page_id\", \"language\"]\n",
    "    )\n",
    "\n",
    "    s = templates_per_timebin.dropna()\n",
    "    s = s[s.apply(len) > 1].explode().str.split(\"=\")\n",
    "    has_two = s.apply(len) == 2\n",
    "    bad_entries = s[~has_two]\n",
    "    if not bad_entries.empty:\n",
    "        logger.warning(bad_entries.tolist())\n",
    "    s = s[has_two]\n",
    "    df = pd.DataFrame.from_records(s, index=s.index, columns=[\"template\", \"count\"])\n",
    "    df[\"count\"] = df[\"count\"].astype(int)\n",
    "    return df.pivot_table(\n",
    "        index=\"timebin\", columns=\"template\", values=\"count\", aggfunc=\"sum\", fill_value=0\n",
    "    )\n",
    "\n",
    "\n",
    "def chunkwise_template_counts(entries: List[pd.DataFrame]):\n",
    "    with mp.Pool(NWORKERS) as pool:\n",
    "        counts = list(\n",
    "            tqdm(pool.imap_unordered(etymology_template_counts, entries), total=SPLIT)\n",
    "        )\n",
    "    all_counts = pd.DataFrame(\n",
    "        0, index=BINS, columns=set(c for d in counts for c in d.columns)\n",
    "    )\n",
    "    for count in counts:\n",
    "        all_counts = all_counts.add(count, fill_value=0)\n",
    "    templates = all_counts.iloc[-1].sort_values(ascending=False)\n",
    "    return all_counts[templates.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d32364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# template_counts = chunkwise_template_counts(entries)\n",
    "# template_counts.to_pickle(DATA_PATH / \"template_counts.pickle\")\n",
    "template_counts = pd.read_pickle(DATA_PATH / \"template_counts.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6a2c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_counts.iloc[-1].sort_values(ascending=False)[:32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee4c5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = template_counts.sum().sort_values(ascending=False).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9d7fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = [template_counts[ts].iloc[:, i * 8 : (i + 1) * 8] for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb444c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "for group, ax in zip(groups, axs.flatten()):\n",
    "    group.plot(ax=ax)\n",
    "    ax.yaxis.set_major_formatter(numberFormatter)\n",
    "plt.savefig(\"templates.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
