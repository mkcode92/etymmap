{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c76c9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "import etymmap.specific_en\n",
    "\n",
    "\n",
    "from etymmap.wiktionary import Wiktionary, MongoEntryStore\n",
    "from etymmap.extraction import init_and_configure\n",
    "\n",
    "from utils.gloss_similarity import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c42b147",
   "metadata": {},
   "outputs": [],
   "source": [
    "etymmap.specific_en.configure()\n",
    "DATA_PATH = Path(\"./data/enwiktionary-20220601-pages-meta-current/\")\n",
    "\n",
    "store = MongoEntryStore.from_config(\n",
    "    {\n",
    "        \"address\": \"mongodb://localhost:27017\",\n",
    "        \"dbname\": \"enwiktionary\",\n",
    "        \"collection\": \"20220601\",\n",
    "    }\n",
    ")\n",
    "enw = Wiktionary(store, default_namespaces=(0, 118))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915d64ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_collector = GlossSimilarityItemCollector(enw, cache=DATA_PATH / \"lexicon.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1622ed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_collector.run(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79da4acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the relevant data\n",
    "\n",
    "train_cols = [\"match\", \"temp_gloss\", \"definition\", \"sense_idx\", \"term\", \"lang\"]\n",
    "gold_std_sense = pd.read_csv(data / \"gold_standard_sense.csv\")[train_cols]\n",
    "gold_std_etym = pd.read_csv(data / \"gold_standard_etym.csv\")[train_cols]\n",
    "# we use previously annotated data here\n",
    "annotated = pd.read_csv(data / \"annotate20210401.csv\")[train_cols]\n",
    "with_value = (annotated.match == 0.0) | (annotated.match == 1.0)\n",
    "not_annotated = annotated[~with_value]\n",
    "annotated = annotated[with_value]\n",
    "annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccb8262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select actual training data here\n",
    "\n",
    "all_data = pd.concat([annotated, gold_std_sense], ignore_index=True)\n",
    "all_data = all_data[all_data.definition.notnull()]\n",
    "len(annotated), len(gold_std_sense), len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9504a788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train/test by entries\n",
    "\n",
    "by_lex = all_data.set_index([\"term\", \"lang\"])\n",
    "all_entries = by_lex.index.drop_duplicates()\n",
    "train_lex = all_entries.to_frame().sample(frac=0.75, random_state=33)\n",
    "train = by_lex.loc[train_lex.index]\n",
    "test = by_lex.loc[~by_lex.index.isin(train_lex.index)]\n",
    "\n",
    "# also, for evaluation, create test set where mapping to the correct sense is considered correct\n",
    "l = []\n",
    "for d in (train, test):\n",
    "    sense_match = (\n",
    "        d.reset_index()\n",
    "        .groupby([\"term\", \"lang\", \"sense_idx\", \"temp_gloss\"])\n",
    "        .match.agg(max)\n",
    "    )\n",
    "    t = d.set_index([\"sense_idx\", \"temp_gloss\"], append=True)\n",
    "    t[\"sense_match\"] = sense_match\n",
    "    l.append(t.reset_index(level=[\"sense_idx\", \"temp_gloss\"]))\n",
    "train, test = l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d377376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of items and entries\n",
    "\n",
    "len(train), len(test), len(train.index.drop_duplicates()), len(\n",
    "    test.index.drop_duplicates()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be780387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# featurize\n",
    "\n",
    "trainF = pd.DataFrame.from_records(\n",
    "    [\n",
    "        featurize(definition, temp_gloss)\n",
    "        for definition, temp_gloss in zip(train.definition, train.temp_gloss)\n",
    "    ]\n",
    ")\n",
    "testF = pd.DataFrame.from_records(\n",
    "    [\n",
    "        featurize(definition, temp_gloss)\n",
    "        for definition, temp_gloss in zip(test.definition, test.temp_gloss)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883e6372",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  evalutate all single-var models\n",
    "_, predictions = get_univariate_predictions(trainF, train, testF, test, cv=10)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02992aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = predictions.groupby(level=0, axis=1).apply(\n",
    "    lambda df: pd.DataFrame(\n",
    "        {\n",
    "            f\"{pred}_{l}\": precision_recall_fscore_support(\n",
    "                compare, df.loc[:, (slice(None), pred)], average=\"binary\"\n",
    "            )[:3]\n",
    "            for l, compare in [(\"pairwise\", test.match), (\"sense\", test.sense_match)]\n",
    "            for pred in [\"pairwise\", \"argmax\"]\n",
    "        },\n",
    "        index=[\"prec\", \"rec\", \"f1\"],\n",
    "    )\n",
    ")\n",
    "performance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40e8d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_tversky, tversky = [\n",
    "    performance.loc[\n",
    "        :,\n",
    "        (\n",
    "            [c for c in performance.columns.get_level_values(0) if c.startswith(pref)],\n",
    "            slice(None),\n",
    "        ),\n",
    "    ]\n",
    "    for pref in [\"fuzzy\", \"tversky\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f43ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_tversky.groupby(level=1, axis=1).apply(\n",
    "    lambda df: df[df.idxmax(axis=1)].set_axis(\n",
    "        [\"best_prec\", \"best_rec\", \"best_f1\"], axis=1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e448b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tversky.groupby(level=1, axis=1).apply(\n",
    "    lambda df: df[df.idxmax(axis=1)].set_axis(\n",
    "        [\"best_prec\", \"best_rec\", \"best_f1\"], axis=1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb1b3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    tversky[\"tversky_0.32\"]\n",
    "    .multiply(100)\n",
    "    .iloc[:, [0, 1, 3]]\n",
    "    .to_latex(float_format=\"{:.2f}\".format)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c859d477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# univariate performances\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5), sharey=True)\n",
    "for i, (label, data) in enumerate(\n",
    "    [\n",
    "        (\"tversky\", tversky.loc[:, (slice(None), \"pairwise_pairwise\")]),\n",
    "        (\"fuzzy tversky\", fuzzy_tversky.loc[:, (slice(None), \"pairwise_pairwise\")]),\n",
    "    ]\n",
    "):\n",
    "    axes[i].set_title(label)\n",
    "    axes[i].plot(data.T.values, label=[\"prec\", \"rec\", \"F1\"])\n",
    "    axes[i].set_xticks([0, 25, 51])\n",
    "    axes[i].set_xlabel(\"alpha\")\n",
    "    axes[i].set_xticklabels([0, 0.5, 1.0])\n",
    "    axes[i].legend()\n",
    "    axes[i].grid()\n",
    "\n",
    "all_features = [f for f in trainF.columns if \"tversky\" not in f] + [\n",
    "    \"tversky_0.32\",\n",
    "    \"fuzzy_tversky_0.06\",\n",
    "]\n",
    "data = performance.loc[:, (all_features, \"pairwise_pairwise\")]\n",
    "axes[2].xaxis.set_major_locator(plt.FixedLocator(range(len(all_features))))\n",
    "for tick in axes[2].get_xticklabels():\n",
    "    tick.set_rotation(90)\n",
    "axes[2].bar(all_features, data.loc[\"prec\"].to_list(), label=\"prec\", width=0.5, zorder=3)\n",
    "axes[2].bar(\n",
    "    all_features,\n",
    "    data.loc[\"rec\"].to_list(),\n",
    "    label=\"rec\",\n",
    "    width=0.5,\n",
    "    align=\"edge\",\n",
    "    zorder=2,\n",
    ")\n",
    "axes[2].legend()\n",
    "axes[2].grid(axis=\"y\", zorder=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007a0909",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_features = [c for c in trainF.columns if \"tversky\" not in c] + [\n",
    "    \"tversky_0.32\",\n",
    "    \"fuzzy_tversky_0.06\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88e883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all with only best tversky\n",
    "all_features = get_evaluation(trainF, train, testF, test, base_features)\n",
    "all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5051adf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no fuzzy features\n",
    "feats = [f for f in base_features if \"levenshtein\" not in f and \"fuzzy\" not in f]\n",
    "no_fuzzy = get_evaluation(trainF, train, testF, test, feats)\n",
    "no_fuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33020332",
   "metadata": {},
   "outputs": [],
   "source": [
    "*_, p = get_multivariate_predictions(trainF, train, testF, test, feats)\n",
    "t2 = pd.concat([test.reset_index(), p], axis=1)\n",
    "t2.set_index([\"term\", \"lang\"], inplace=True)\n",
    "# get all entries surrounding errors\n",
    "t2.loc[\n",
    "    t2.loc[~(p.argmax.astype(bool).values) & t2.match.astype(bool).values].index\n",
    "].to_csv(DATA_PATH / \"false_negatives.csv\")\n",
    "t2.loc[\n",
    "    t2.loc[(p.argmax.astype(bool).values) & ~t2.match.astype(bool).values].index\n",
    "].to_csv(DATA_PATH / \"false_positives.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c566f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only cutoff levenshtein\n",
    "feats = [f for f in base_features if f not in {\"char_levenshtein\", \"word_levenshtein\"}]\n",
    "get_evaluation(trainF, train, testF, test, feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5a06d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only character features\n",
    "feats = [f for f in base_features if f.startswith(\"char\")]\n",
    "get_evaluation(trainF, train, testF, test, feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e765fad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only tversky\n",
    "get_evaluation(trainF, train, testF, test, [\"tversky_0.32\", \"fuzzy_tversky_0.06\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eba1b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no tversky\n",
    "feats = [f for f in base_features if \"tversky\" not in f]\n",
    "get_evaluation(trainF, train, testF, test, feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46af972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only ratios and lcsm\n",
    "feats = [f for f in base_features if \"ratio\" in f or \"longest_match\" in f]\n",
    "get_evaluation(trainF, train, testF, test, feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca210bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export models\n",
    "\n",
    "all_train = pd.DataFrame.from_records(\n",
    "    [\n",
    "        featurize(definition, temp_gloss)\n",
    "        for definition, temp_gloss in zip(all_data.definition, all_data.temp_gloss)\n",
    "    ]\n",
    ")\n",
    "\n",
    "for name, features in (\"all_features\", base_features), (\n",
    "    \"no_fuzzy\",\n",
    "    [f for f in base_features if \"levenshtein\" not in f and \"fuzzy\" not in f],\n",
    "):\n",
    "    train_ = all_train[features]\n",
    "    scaler = StandardScaler().fit(train_)\n",
    "    params = dict(\n",
    "        Cs=10,\n",
    "        cv=10,\n",
    "        random_state=33,\n",
    "        solver=\"lbfgs\",\n",
    "        multi_class=\"ovr\",\n",
    "        max_iter=100,\n",
    "        class_weight=\"balanced\",\n",
    "    )\n",
    "    LR = LogisticRegressionCV(**params)\n",
    "    LR.fit(scaler.transform(train_), all_data.match)\n",
    "    with open(DATA_PATH / f\"{name}.model\", \"wb\") as dest:\n",
    "        pickle.dump(LR, dest)\n",
    "    with open(DATA_PATH / f\"{name}.scaler\", \"wb\") as dest:\n",
    "        pickle.dump(scaler, dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5896a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other models\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "scaler = StandardScaler().fit(trainF[base_features])\n",
    "clf = tree.DecisionTreeClassifier().fit(\n",
    "    scaler.transform(trainF[base_features]), train.match\n",
    ")\n",
    "multi_tree_predictions = clf.predict(scaler.transform(testF[base_features]))\n",
    "precision_recall_fscore_support(test.match, multi_tree_predictions, average=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c020597c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "scaler = StandardScaler().fit(trainF[base_features])\n",
    "clf = RandomForestClassifier().fit(scaler.transform(trainF[base_features]), train.match)\n",
    "multi_tree_predictions = clf.predict(scaler.transform(testF[base_features]))\n",
    "precision_recall_fscore_support(test.match, multi_tree_predictions, average=\"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d72417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "scaler = StandardScaler().fit(trainF[base_features])\n",
    "clf = svm.SVC().fit(scaler.transform(trainF[base_features]), train.match)\n",
    "multi_tree_predictions = clf.predict(scaler.transform(testF[base_features]))\n",
    "precision_recall_fscore_support(test.match, multi_tree_predictions, average=\"binary\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
